import "server-only";

import type { StreamChunk, StreamInput } from "./types";

function estimateTokens(text: string): number {
  return Math.max(1, Math.ceil(text.length / 4));
}

/**
 * Gigaviz AI (local) streaming adapter.
 * Emulates streaming by chunking a generated response.
 * In production, this could call an internal LLM endpoint.
 */
export async function* streamLocal(input: StreamInput): AsyncGenerator<StreamChunk, void, unknown> {
  // Simulate processing delay
  await new Promise((resolve) => setTimeout(resolve, 100));

  // Check for abort before generating
  if (input.signal?.aborted) {
    yield { done: true };
    return;
  }

  // Generate a helpful response based on the mode
  let response: string;
  const promptPreview = input.prompt.slice(0, 300);

  switch (input.mode) {
    case "copy":
      response = `Here's a refined version of your text:\n\n${promptPreview}...\n\n(Rewritten for clarity and conciseness by Gigaviz AI)`;
      break;
    case "summary":
      response = `**Summary:**\n\n• Key point from the provided content\n• Another important aspect\n• Final takeaway\n\n(Generated by Gigaviz AI based on: "${promptPreview.slice(0, 50)}...")`;
      break;
    default:
      response = `Thank you for your message! Here's my response:\n\n${promptPreview}\n\n(Processed by Gigaviz AI - your local assistant)`;
  }

  // Simulate streaming by yielding chunks
  const chunkSize = 15; // Characters per chunk
  const chunks: string[] = [];
  for (let i = 0; i < response.length; i += chunkSize) {
    chunks.push(response.slice(i, i + chunkSize));
  }

  for (const chunk of chunks) {
    // Check for abort between chunks
    if (input.signal?.aborted) {
      yield { done: true };
      return;
    }

    // Simulate network delay
    await new Promise((resolve) => setTimeout(resolve, 20));
    yield { delta: chunk, provider: "local" };
  }

  const inputTokens = estimateTokens(input.prompt);
  const outputTokens = estimateTokens(response);

  yield {
    done: true,
    provider: "local",
    usage: {
      inputTokens,
      outputTokens,
      totalTokens: inputTokens + outputTokens,
    },
  };
}
